{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20eac5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n### Q1. Min-Max Scaling in Data Preprocessing\\n\\n**Min-Max Scaling**: A normalization technique used to transform features to a fixed range, usually [0, 1] or [-1, 1]. This technique helps in bringing all features to the same scale without distorting differences in the ranges of values.\\n\\n**Formula**:\\n\\\\[ X_{\\text{scaled}} = \\x0crac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\\\]\\n\\n**Example**:\\n```python\\nimport numpy as np\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# Sample data\\ndata = np.array([[1], [5], [10], [15], [20]])\\n\\n# Min-Max scaling to range [0, 1]\\nscaler = MinMaxScaler(feature_range=(0, 1))\\nscaled_data = scaler.fit_transform(data)\\nprint(scaled_data)\\n```\\n\\n### Q2. Unit Vector Technique in Feature Scaling\\n\\n**Unit Vector Technique**: Also known as normalization to unit norm, it scales the feature vector to have a unit norm (i.e., the length of the vector is 1). This is useful when the direction of the data points matters more than their magnitude.\\n\\n**Formula**:\\n\\\\[ \\text{X}_{\\text{norm}} = \\x0crac{X}{\\\\|X\\\\|} \\\\]\\nwhere \\\\(\\\\|X\\\\|\\\\) is the Euclidean norm of the vector \\\\(X\\\\).\\n\\n**Example**:\\n```python\\nfrom sklearn.preprocessing import normalize\\n\\n# Sample data\\ndata = np.array([[1, 2], [3, 4], [5, 6]])\\n\\n# Unit vector normalization\\nnormalized_data = normalize(data, norm='l2')\\nprint(normalized_data)\\n```\\n\\n**Difference from Min-Max Scaling**: Min-Max scaling transforms data to a fixed range, while unit vector scaling transforms data to have unit length, emphasizing direction rather than magnitude.\\n\\n### Q3. Principal Component Analysis (PCA)\\n\\n**PCA**: A dimensionality reduction technique that transforms the original features into a new set of uncorrelated features called principal components, ordered by the amount of variance they capture from the data.\\n\\n**Usage**:\\n1. **Dimensionality Reduction**: Reduces the number of features while retaining most of the variance in the data.\\n2. **Feature Extraction**: Identifies and combines the most significant features.\\n\\n**Example**:\\n```python\\nimport numpy as np\\nfrom sklearn.decomposition import PCA\\n\\n# Sample data\\ndata = np.array([[1, 2], [3, 4], [5, 6]])\\n\\n# Applying PCA\\npca = PCA(n_components=1)  # Reduce to 1 dimension\\nreduced_data = pca.fit_transform(data)\\nprint(reduced_data)\\n```\\n\\n### Q4. PCA and Feature Extraction\\n\\n**Relationship**:\\n- **Feature Extraction**: PCA can create new features (principal components) that are linear combinations of the original features, focusing on capturing the maximum variance.\\n\\n**Using PCA for Feature Extraction**:\\n1. **Compute Principal Components**: Identify directions (components) that maximize variance.\\n2. **Transform Data**: Project the original data onto these new directions.\\n\\n**Example**:\\n```python\\nimport numpy as np\\nfrom sklearn.decomposition import PCA\\n\\n# Sample data\\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n\\n# Applying PCA\\npca = PCA(n_components=2)  # Extract 2 principal components\\nprincipal_components = pca.fit_transform(data)\\nprint(principal_components)\\n```\\n\\n### Q5. Min-Max Scaling for a Food Delivery Service Dataset\\n\\n**Application**:\\n1. **Identify Features**: Price, rating, delivery time.\\n2. **Apply Min-Max Scaling**: Normalize each feature to a range of [0, 1] to ensure comparability and improve model performance.\\n\\n**Example**:\\n```python\\nimport pandas as pd\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# Sample data\\ndata = pd.DataFrame({\\n    'price': [10, 20, 30, 40, 50],\\n    'rating': [3, 4, 5, 2, 1],\\n    'delivery_time': [30, 25, 20, 15, 10]\\n})\\n\\n# Min-Max scaling\\nscaler = MinMaxScaler()\\nscaled_data = scaler.fit_transform(data)\\nprint(scaled_data)\\n```\\n\\n### Q6. Using PCA to Reduce Dimensionality for Stock Price Prediction\\n\\n**Steps**:\\n1. **Standardize Data**: Ensure each feature has zero mean and unit variance.\\n2. **Apply PCA**: Reduce the dimensionality while retaining the majority of variance.\\n\\n**Example**:\\n```python\\nimport numpy as np\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Sample data\\ndata = np.array([\\n    [10, 200, 0.5],\\n    [12, 220, 0.6],\\n    [11, 210, 0.55],\\n    [13, 230, 0.65]\\n])\\n\\n# Standardize data\\nscaler = StandardScaler()\\nscaled_data = scaler.fit_transform(data)\\n\\n# Applying PCA\\npca = PCA(n_components=2)  # Reduce to 2 dimensions\\nreduced_data = pca.fit_transform(scaled_data)\\nprint(reduced_data)\\n```\\n\\n### Q7. Min-Max Scaling for Values [1, 5, 10, 15, 20] to Range [-1, 1]\\n\\n**Formula**:\\n\\\\[ X_{\\text{scaled}} = 2 \\\\left(\\x0crac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\right) - 1 \\\\]\\n\\n**Application**:\\n```python\\nimport numpy as np\\n\\n# Sample data\\ndata = np.array([1, 5, 10, 15, 20])\\n\\n# Min-Max scaling to range [-1, 1]\\nX_min, X_max = data.min(), data.max()\\nscaled_data = 2 * (data - X_min) / (X_max - X_min) - 1\\nprint(scaled_data)\\n```\\n\\n### Q8. Feature Extraction Using PCA for Dataset with Features: [height, weight, age, gender, blood pressure]\\n\\n**Steps**:\\n1. **Standardize Data**: Convert features to zero mean and unit variance.\\n2. **Apply PCA**: Determine the number of principal components to retain based on explained variance.\\n\\n**Choosing Principal Components**: Retain enough components to explain a significant portion of the variance (e.g., 95%).\\n\\n**Example**:\\n```python\\nimport numpy as np\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Sample data\\ndata = np.array([\\n    [180, 75, 25, 0, 120],  # height, weight, age, gender, blood pressure\\n    [160, 65, 30, 1, 110],\\n    [170, 70, 28, 0, 115],\\n    [175, 80, 35, 1, 125]\\n])\\n\\n# Standardize data\\nscaler = StandardScaler()\\nscaled_data = scaler.fit_transform(data)\\n\\n# Applying PCA\\npca = PCA()\\npca.fit(scaled_data)\\n\\n# Explained variance\\nexplained_variance = pca.explained_variance_ratio_\\ncumulative_variance = np.cumsum(explained_variance)\\n\\n# Determine number of components to retain (e.g., for 95% variance)\\nn_components = np.argmax(cumulative_variance >= 0.95) + 1\\n\\n# Transform data using selected number of components\\npca = PCA(n_components=n_components)\\nreduced_data = pca.fit_transform(scaled_data)\\nprint(reduced_data)\\n```\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "### Q1. Min-Max Scaling in Data Preprocessing\n",
    "\n",
    "**Min-Max Scaling**: A normalization technique used to transform features to a fixed range, usually [0, 1] or [-1, 1]. This technique helps in bringing all features to the same scale without distorting differences in the ranges of values.\n",
    "\n",
    "**Formula**:\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1], [5], [10], [15], [20]])\n",
    "\n",
    "# Min-Max scaling to range [0, 1]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "```\n",
    "\n",
    "### Q2. Unit Vector Technique in Feature Scaling\n",
    "\n",
    "**Unit Vector Technique**: Also known as normalization to unit norm, it scales the feature vector to have a unit norm (i.e., the length of the vector is 1). This is useful when the direction of the data points matters more than their magnitude.\n",
    "\n",
    "**Formula**:\n",
    "\\[ \\text{X}_{\\text{norm}} = \\frac{X}{\\|X\\|} \\]\n",
    "where \\(\\|X\\|\\) is the Euclidean norm of the vector \\(X\\).\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Unit vector normalization\n",
    "normalized_data = normalize(data, norm='l2')\n",
    "print(normalized_data)\n",
    "```\n",
    "\n",
    "**Difference from Min-Max Scaling**: Min-Max scaling transforms data to a fixed range, while unit vector scaling transforms data to have unit length, emphasizing direction rather than magnitude.\n",
    "\n",
    "### Q3. Principal Component Analysis (PCA)\n",
    "\n",
    "**PCA**: A dimensionality reduction technique that transforms the original features into a new set of uncorrelated features called principal components, ordered by the amount of variance they capture from the data.\n",
    "\n",
    "**Usage**:\n",
    "1. **Dimensionality Reduction**: Reduces the number of features while retaining most of the variance in the data.\n",
    "2. **Feature Extraction**: Identifies and combines the most significant features.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=1)  # Reduce to 1 dimension\n",
    "reduced_data = pca.fit_transform(data)\n",
    "print(reduced_data)\n",
    "```\n",
    "\n",
    "### Q4. PCA and Feature Extraction\n",
    "\n",
    "**Relationship**:\n",
    "- **Feature Extraction**: PCA can create new features (principal components) that are linear combinations of the original features, focusing on capturing the maximum variance.\n",
    "\n",
    "**Using PCA for Feature Extraction**:\n",
    "1. **Compute Principal Components**: Identify directions (components) that maximize variance.\n",
    "2. **Transform Data**: Project the original data onto these new directions.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=2)  # Extract 2 principal components\n",
    "principal_components = pca.fit_transform(data)\n",
    "print(principal_components)\n",
    "```\n",
    "\n",
    "### Q5. Min-Max Scaling for a Food Delivery Service Dataset\n",
    "\n",
    "**Application**:\n",
    "1. **Identify Features**: Price, rating, delivery time.\n",
    "2. **Apply Min-Max Scaling**: Normalize each feature to a range of [0, 1] to ensure comparability and improve model performance.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({\n",
    "    'price': [10, 20, 30, 40, 50],\n",
    "    'rating': [3, 4, 5, 2, 1],\n",
    "    'delivery_time': [30, 25, 20, 15, 10]\n",
    "})\n",
    "\n",
    "# Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "```\n",
    "\n",
    "### Q6. Using PCA to Reduce Dimensionality for Stock Price Prediction\n",
    "\n",
    "**Steps**:\n",
    "1. **Standardize Data**: Ensure each feature has zero mean and unit variance.\n",
    "2. **Apply PCA**: Reduce the dimensionality while retaining the majority of variance.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = np.array([\n",
    "    [10, 200, 0.5],\n",
    "    [12, 220, 0.6],\n",
    "    [11, 210, 0.55],\n",
    "    [13, 230, 0.65]\n",
    "])\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "reduced_data = pca.fit_transform(scaled_data)\n",
    "print(reduced_data)\n",
    "```\n",
    "\n",
    "### Q7. Min-Max Scaling for Values [1, 5, 10, 15, 20] to Range [-1, 1]\n",
    "\n",
    "**Formula**:\n",
    "\\[ X_{\\text{scaled}} = 2 \\left(\\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\right) - 1 \\]\n",
    "\n",
    "**Application**:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Min-Max scaling to range [-1, 1]\n",
    "X_min, X_max = data.min(), data.max()\n",
    "scaled_data = 2 * (data - X_min) / (X_max - X_min) - 1\n",
    "print(scaled_data)\n",
    "```\n",
    "\n",
    "### Q8. Feature Extraction Using PCA for Dataset with Features: [height, weight, age, gender, blood pressure]\n",
    "\n",
    "**Steps**:\n",
    "1. **Standardize Data**: Convert features to zero mean and unit variance.\n",
    "2. **Apply PCA**: Determine the number of principal components to retain based on explained variance.\n",
    "\n",
    "**Choosing Principal Components**: Retain enough components to explain a significant portion of the variance (e.g., 95%).\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = np.array([\n",
    "    [180, 75, 25, 0, 120],  # height, weight, age, gender, blood pressure\n",
    "    [160, 65, 30, 1, 110],\n",
    "    [170, 70, 28, 0, 115],\n",
    "    [175, 80, 35, 1, 125]\n",
    "])\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Determine number of components to retain (e.g., for 95% variance)\n",
    "n_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "# Transform data using selected number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "reduced_data = pca.fit_transform(scaled_data)\n",
    "print(reduced_data)\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a4a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
